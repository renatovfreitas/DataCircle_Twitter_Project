{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)  # Adjust the width to display everything\n",
    "pd.set_option('display.max_colwidth', None)  # Show full column content\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "import fasttext\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv files\n",
    "biden_df = pd.read_csv(r\"C:\\Users\\User\\iCloudDrive\\Cursos\\Data Circle\\DataCircle_Twitter_Project\\hashtag_joebiden.csv\",lineterminator='\\n')\n",
    "trump_df = pd.read_csv(r\"C:\\Users\\User\\iCloudDrive\\Cursos\\Data Circle\\DataCircle_Twitter_Project\\hashtag_donaldtrump.csv\", lineterminator='\\n')\n",
    "\n",
    "print(biden_df.info())\n",
    "print(trump_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column 'candidate' to differentiate tweets of each candidate after concatenation \n",
    "trump_df['candidate'] = 'trump'\n",
    "\n",
    "# biden dataframe \n",
    "biden_df['candidate'] = 'biden'\n",
    "\n",
    "# combining the dataframes \n",
    "twitter_df = pd.concat([trump_df, biden_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove irrelevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant data (\"user_name\", \"user_screen_name\", \"user_description\")\n",
    "twitter_df = twitter_df[['created_at', 'tweet_id', 'tweet', 'likes', 'retweet_count', 'source',\n",
    "       'user_id', 'user_join_date', 'user_followers_count', 'user_location', 'city', 'country', 'state', 'candidate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on 'tweet_id', 'tweet', and 'created_at'\n",
    "twitter_df = twitter_df[~(twitter_df.duplicated(subset=['tweet_id', 'tweet', 'created_at'], keep=False))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure data consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime\n",
    "columns_to_datetime = ['created_at', 'user_join_date']\n",
    "twitter_df[columns_to_datetime] = twitter_df[columns_to_datetime].apply(pd.to_datetime, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert selected columns to int\n",
    "def convert_columns_to_int(df, columns):\n",
    "    for column in columns:\n",
    "        df[column] = pd.to_numeric(df[column], errors='coerce').astype('int64') \n",
    "    return df\n",
    "\n",
    "columns_to_convert_int = ['likes', 'retweet_count', 'user_followers_count', 'tweet_id', \"user_id\"]\n",
    "twitter_df = convert_columns_to_int(twitter_df, columns_to_convert_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the tweet column\n",
    "def clean_tweet_column(df, tweet_column):\n",
    "\n",
    "    # Work on a copy of the tweet column to avoid modifying the original\n",
    "    tweet_cleaned = df[tweet_column].copy()\n",
    "\n",
    "    # Clean and convert to lowercase\n",
    "    tweet_cleaned = tweet_cleaned.str.lower().str.strip()\n",
    "\n",
    "    # Replace '\\n' with space ' '\n",
    "    tweet_cleaned = tweet_cleaned.str.replace('\\n', ' ')\n",
    "\n",
    "    # Remove URLs\n",
    "    url_pattern = r'http[s]?://\\S+|www\\.\\S+'\n",
    "    tweet_cleaned = tweet_cleaned.str.replace(url_pattern, '', regex=True)\n",
    "\n",
    "    # Remove unwanted symbols (keeping letters, numbers, hashtags, and spaces)\n",
    "    tweet_cleaned = tweet_cleaned.str.replace(r'[^a-zA-Z0-9# ]', '', regex=True)\n",
    "\n",
    "    return tweet_cleaned\n",
    "\n",
    "# Apply the function and assign the cleaned result to 'tweet_cleaned' without modifying 'tweet'\n",
    "twitter_df[\"tweet_cleaned\"] = clean_tweet_column(twitter_df, 'tweet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all text columns are lowercase for consistent NLP analysis.\n",
    "def clean_and_convert_text_columns(df, text_columns):\n",
    "    # Clean and convert text columns to lowercase\n",
    "    for column in text_columns:\n",
    "        df[column] = df[column].str.lower().str.strip()  # Convert to lowercase, remove leading and trailing spaces\n",
    "\n",
    "    return df\n",
    "\n",
    "# Return the names of object columns\n",
    "text_columns_to_convert = [\"source\", \"user_location\", \"city\", \"state\", \"country\"]\n",
    "twitter_df = clean_and_convert_text_columns(twitter_df, text_columns_to_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df[\"country\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping dictionary\n",
    "country_mapping = {\n",
    "    'united states of america': 'united states',\n",
    "    'the netherlands': 'netherlands'\n",
    "    }\n",
    "\n",
    "# Standardize country names using the mapping\n",
    "twitter_df['country'] = twitter_df['country'].replace(country_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in the 'source' column with the most frequent value (mode)\n",
    "twitter_df[\"source\"] = twitter_df[\"source\"].fillna(twitter_df[\"source\"].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where user location is null and either lat, long, city, state or country is not null\n",
    "twitter_df[(twitter_df[\"user_location\"].isna()) & (~(twitter_df[\"city\"].isna()) | ~(twitter_df[\"state\"].isna()) | ~(twitter_df[\"country\"].isna()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous output, whenever the user location is null, lat, long, city, state and country will also be null. Which means user location column can not be filled with the other location columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null location columns with \"unknown\"\n",
    "twitter_df[\"user_location\"] = twitter_df[\"user_location\"].fillna(\"unkown\")\n",
    "twitter_df[\"city\"] = twitter_df[\"city\"].fillna(\"unkown\")\n",
    "twitter_df[\"state\"] = twitter_df[\"state\"].fillna(\"unkown\")\n",
    "twitter_df[\"country\"] = twitter_df[\"country\"].fillna(\"unkown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect tweet language and filter only english tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FastText pre-trained language identification model\n",
    "model = fasttext.load_model('lid.176.bin')\n",
    "\n",
    "# Function to detect language\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        predictions = model.predict(text, k=1)  # Get top 1 language prediction\n",
    "        lang_code = predictions[0][0].split(\"__label__\")[1]  # Extract language code\n",
    "        return lang_code\n",
    "    except Exception as e:\n",
    "        return 'unknown'  # Return 'unknown' for any errors\n",
    "\n",
    "# Apply language detection to your \"tweet\" column \n",
    "twitter_df['language'] = twitter_df['tweet_cleaned'].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter only twitter in english language\n",
    "twitter_df = twitter_df[twitter_df['language']=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop language column\n",
    "twitter_df = twitter_df.drop(columns='language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of stop words in English\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define a function to remove stop words\n",
    "def remove_stopwords(text):\n",
    "     # Split the text into words by spaces, filter out stop words, and rejoin\n",
    "    filtered_words = [word for word in text.split() if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply the function to the 'tweet' column\n",
    "twitter_df['tweet_cleaned'] = twitter_df['tweet_cleaned'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df[[\"tweet\", \"tweet_cleaned\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original tweet column\n",
    "twitter_df = twitter_df.drop(columns=\"tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"likes skewness:\")\n",
    "print(twitter_df[\"likes\"].skew())\n",
    "print(\"\\n\")\n",
    "print(\"likes kurtosis:\")\n",
    "print(twitter_df[\"likes\"].kurtosis())\n",
    "print(\"\\n\")\n",
    "print(\"retweet skewness:\")\n",
    "print(twitter_df[\"retweet_count\"].skew())\n",
    "print(\"\\n\")\n",
    "print(\"retweet kurtosis:\")\n",
    "print(twitter_df[\"retweet_count\"].kurtosis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data for 'likes' and 'retweets'\n",
    "likes = twitter_df['likes']\n",
    "retweets = twitter_df['retweet_count']\n",
    "\n",
    "# 1. Histogram and KDE for 'likes' and 'retweets'\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(likes, bins=50, kde=True)\n",
    "plt.title('Likes Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(retweets, bins=50, kde=True)\n",
    "plt.title('Retweets Distribution')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 2. Boxplot for 'likes' and 'retweets' (helps identify outliers)\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(x=likes)\n",
    "plt.title('Boxplot of Likes')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x=retweets)\n",
    "plt.title('Boxplot of Retweets')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using IQR for 'likes' and 'retweets'\n",
    "\n",
    "# For 'likes'\n",
    "Q1_likes = twitter_df['likes'].quantile(0.25)\n",
    "Q3_likes = twitter_df['likes'].quantile(0.75)\n",
    "IQR_likes = Q3_likes - Q1_likes\n",
    "\n",
    "lower_bound_likes = Q1_likes - 3 * IQR_likes\n",
    "upper_bound_likes = Q3_likes + 3 * IQR_likes\n",
    "\n",
    "# For 'retweets'\n",
    "Q1_retweets = twitter_df['retweet_count'].quantile(0.25)\n",
    "Q3_retweets = twitter_df['retweet_count'].quantile(0.75)\n",
    "IQR_retweets = Q3_retweets - Q1_retweets\n",
    "\n",
    "lower_bound_retweets = Q1_retweets - 3 * IQR_retweets\n",
    "upper_bound_retweets = Q3_retweets + 3 * IQR_retweets\n",
    "\n",
    "# Identify outliers in 'likes' and 'retweets'\n",
    "outliers_likes = twitter_df[(twitter_df['likes'] < lower_bound_likes) | (twitter_df['likes'] > upper_bound_likes)]\n",
    "outliers_retweets = twitter_df[(twitter_df['retweet_count'] < lower_bound_retweets) | (twitter_df['retweet_count'] > upper_bound_retweets)]\n",
    "\n",
    "print(f\"Number of outliers in 'likes' using IQR: {len(outliers_likes)}\")\n",
    "print(f\"Number of outliers in 'retweets' using IQR: {len(outliers_retweets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outliers_likes[\"likes\"].describe())\n",
    "print(\"\\n\")\n",
    "print(outliers_retweets[\"retweet_count\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "Likes:\n",
    "* The number of outliers decreased after using the 3x IQR factor, which means the bound have been tightened and fewer low outliers have been excluded .\n",
    "* The mean, median, and standard deviation suggest that many of the outliers are still relatively small, but there are a few extreme values (as indicated by the high max of 165,702).\n",
    "\n",
    "Retweets:\n",
    "* The retweet outliers remain unchanged with the 3x IQR factor, indicating that the distribution of retweets may not be as heavily skewed at the lower end. \n",
    "* The presence of very low outliers (min = 1) suggests that many entries are being flagged due to the distribution's natural skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df.to_csv(\"twitter_cleaned_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
