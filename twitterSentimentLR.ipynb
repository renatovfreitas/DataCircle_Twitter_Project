{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renatovfreitas/DataCircle_Twitter_Project/blob/task/twitterSentimentLR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "DYt49AXvCbZy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import twitter_samples, stopwords\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import re # Import the re module for regular expressions\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import TweetTokenizer  # Import TweetTokenizer\n",
        "from nltk.stem import PorterStemmer  # Import PorterStemmer\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3xUF97nLWQk",
        "outputId": "50a262be-d626-4c74-d60a-4ca144a23ae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZLnQC6cEcZB"
      },
      "source": [
        "Pre-processing of tweets include the following steps:\n",
        "\n",
        "1.   Removing punctuations, hyperlinks and hashtags\n",
        "2.   Tokenization — Converting a sentence into list of words\n",
        "3. Converting words to lower cases\n",
        "4. Removing stop words\n",
        "5. Lemmatization/stemming — Transforming a word to its root word\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "-FPNqf0dcfev",
        "outputId": "d2cd962c-2693-4647-8762-944a0faabf57"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8400b170-fbb4-4346-a620-2918ae931382\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8400b170-fbb4-4346-a620-2918ae931382\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving thousand_tweets.csv to thousand_tweets (2).csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"thousand_tweets.csv\")"
      ],
      "metadata": {
        "id": "YTOgC_d6dAqn"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-6zK58meqR9",
        "outputId": "be4c91c3-c853-4376-c884-71f6a2dcae03"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            created_at             tweet_id  likes  retweet_count  \\\n",
            "0  2020-10-15 07:15:00  1316638690953891840      0              0   \n",
            "1  2020-11-07 14:20:18  1325080641336193024      1              0   \n",
            "2  2020-10-16 12:42:31  1317083500428759040      2              1   \n",
            "3  2020-11-07 16:39:47  1325115746138124288     17              0   \n",
            "4  2020-10-25 14:11:58  1320367504141660160    238             61   \n",
            "\n",
            "                source    user_id       user_join_date  user_followers_count  \\\n",
            "0      twitter web app  178730312  2010-08-15 14:56:43                   642   \n",
            "1   twitter for iphone   37132337  2009-05-02 03:44:26                   231   \n",
            "2  twitter for android  371229186  2011-09-10 12:59:32                  1293   \n",
            "3     twitter for ipad  407826450  2011-11-08 16:15:48                   631   \n",
            "4      twitter web app   63227112  2009-08-05 19:21:04                 31871   \n",
            "\n",
            "              user_location           city         country       state  \\\n",
            "0    wherever i may roam...         unkown          unkown      unkown   \n",
            "1                 somewhere         unkown          unkown      unkown   \n",
            "2  san francisco, singapore  san francisco   united states  california   \n",
            "3                   england         unkown  united kingdom     england   \n",
            "4      rt not endorsements.         unkown          unkown      unkown   \n",
            "\n",
            "  candidate                                      tweet_cleaned  \n",
            "0     trump  trade deal #trump could open #nhs us corporati...  \n",
            "1     biden  lilduval #biden america act like theyre scared...  \n",
            "2     trump  wouldnt denounce #qanon poisonous conspiracy t...  \n",
            "3     biden  2020 might redeemed #americadecides2020 ##bide...  \n",
            "4     biden  trump amp co gave chief staff said explicitly ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the Tweets: Use the process_tweet function to clean and tokenize the tweets in DataFrame."
      ],
      "metadata": {
        "id": "DLxTOXZRd41_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check for NaN values and convert to string"
      ],
      "metadata": {
        "id": "YOF5oW0kCBnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBc8tIjt9_NY",
        "outputId": "f2573ca2-f594-46ac-da09-64907a5b3c55"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['created_at', 'tweet_id', 'likes', 'retweet_count', 'source', 'user_id',\n",
            "       'user_join_date', 'user_followers_count', 'user_location', 'city',\n",
            "       'country', 'state', 'candidate', 'tweet_cleaned'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the process_tweet function"
      ],
      "metadata": {
        "id": "KRk2Z4_7eNKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tweet(tweet):\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)  # Remove dollar signs and words following them\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)  # Remove URLs\n",
        "    tweet = re.sub(r'#', '', tweet)  # Remove hash symbol\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    stemmer = PorterStemmer()\n",
        "    tweets_stem = []\n",
        "\n",
        "    for word in tweet_tokens:\n",
        "        if word not in stopwords_english and word not in string.punctuation:\n",
        "            stem_word = stemmer.stem(word)\n",
        "            tweets_stem.append(stem_word)\n",
        "\n",
        "    return tweets_stem"
      ],
      "metadata": {
        "id": "i4zqFiPAdra1"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process the tweets in your DataFrame"
      ],
      "metadata": {
        "id": "7P51bwrKfhs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['processed_tweets'] = df['tweet_cleaned'].apply(process_tweet)\n",
        "print(df[['tweet_cleaned', 'processed_tweets']].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z8HQ0SPfgXA",
        "outputId": "1cac212c-6e36-44a0-bcf7-792a55de5f94"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       tweet_cleaned  \\\n",
            "0  trade deal #trump could open #nhs us corporati...   \n",
            "1  lilduval #biden america act like theyre scared...   \n",
            "2  wouldnt denounce #qanon poisonous conspiracy t...   \n",
            "3  2020 might redeemed #americadecides2020 ##bide...   \n",
            "4  trump amp co gave chief staff said explicitly ...   \n",
            "\n",
            "                                    processed_tweets  \n",
            "0  [trade, deal, trump, could, open, nh, us, corp...  \n",
            "1  [lilduv, biden, america, act, like, theyr, sca...  \n",
            "2  [wouldnt, denounc, qanon, poison, conspiraci, ...  \n",
            "3  [2020, might, redeem, americadecid, 2020, bide...  \n",
            "4  [trump, amp, co, gave, chief, staff, said, exp...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment polarity function\n",
        "def get_polarity(text):\n",
        "    analysis = TextBlob(text)\n",
        "    polarity = analysis.sentiment.polarity\n",
        "    return polarity\n",
        "\n",
        "# Apply sentiment polarity function to the DataFrame\n",
        "df['polarity'] = df['tweet_cleaned'].apply(get_polarity)"
      ],
      "metadata": {
        "id": "fmQVxZFpCB_T"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subjectivity function\n",
        "def get_subjectivity(text):\n",
        "    analysis = TextBlob(text)\n",
        "    if analysis.sentiment.subjectivity < 0.5:\n",
        "        subjectivity = 'objective'\n",
        "    else:\n",
        "        subjectivity = 'subjective'\n",
        "\n",
        "    return subjectivity\n",
        "\n",
        "# Apply subjectivity function to the DataFrame\n",
        "df['subjectivity'] = df['tweet_cleaned'].apply(get_subjectivity)"
      ],
      "metadata": {
        "id": "6r4jL5m2_7Mf"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a column for Sentiment analysis\n",
        "df['sentiment'] = df['polarity'].apply(lambda i: 'positive' if i > 0 else ('neutral' if i == 0 else 'negative'))"
      ],
      "metadata": {
        "id": "ojGd26yi_-ok"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a set of unique words\n"
      ],
      "metadata": {
        "id": "P_eEB71SpRK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "for tweet in df ['processed_tweets']:\n",
        "  all_words.extend(tweet)\n",
        "word_l = list(set(all_words))"
      ],
      "metadata": {
        "id": "AvjF0xyLpTek"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create feature matrix X and labels y\n",
        "# Initialize the feature matrix\n"
      ],
      "metadata": {
        "id": "WmE-Pcy7qAVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.zeros((len(df), len(word_l) +1)) # +1 for bias term\n",
        "# Create the labels based on the sentiment column\n",
        "df['sentiment_label'] = df['sentiment'].map({'positive': 1, 'negative': 0, 'neutral': 0.5})\n",
        "y = df['sentiment_label'].values.reshape(-1, 1)  # Reshape y to be (m, 1)"
      ],
      "metadata": {
        "id": "1V-0InuPqD3_"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill the feature matrix\n",
        "for i, tweet in enumerate(df['processed_tweets']):\n",
        "    for word in tweet:\n",
        "        if word in word_l:\n",
        "            index = word_l.index(word)\n",
        "            X[i, index + 1] += 1  # +1 for the bias term\n",
        "\n",
        "# Add bias term (column of ones)\n",
        "X[:, 0] = 1  # Bias term\n"
      ],
      "metadata": {
        "id": "h26sqVcw1ghX"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize theta\n",
        "theta = np.zeros((X_train.shape[1], 1))  # Initialize theta with zeros"
      ],
      "metadata": {
        "id": "yQIvfHOrwAu0"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the sigmoid and gradient descent functions\n"
      ],
      "metadata": {
        "id": "Dr0qm5ZZre5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def gradientDescent(X, y, theta, alpha, num_iters):\n",
        "    m = len(y)  # Number of training examples\n",
        "    J_history = []  # To store the cost at each iteration\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        z = np.dot(X, theta)  # Linear combination\n",
        "        h = sigmoid(z)  # Hypothesis\n",
        "        J = (-1/m) * (np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1 - h + 1e-15)))  # Cost function\n",
        "        grad = (1/m) * np.dot(X.T, (h - y))  # Gradient\n",
        "        theta = theta - alpha * grad  # Update theta\n",
        "        J_history.append(J[0, 0])  # Store the cost\n",
        "\n",
        "    return theta, J_history"
      ],
      "metadata": {
        "id": "kL9DxUforfyI"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"x shape:\", X.shape)  # Should be (m, n)\n",
        "print(\"y shape:\", y.shape)  # Should be (m,) or (m, 1)\n",
        "print(\"theta shape:\", theta.shape)  # Should be (n,) or (n, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fruByIM5xGIg",
        "outputId": "92707704-fcc1-4d24-caf2-e11014322d30"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x shape: (1000, 4582)\n",
            "y shape: (1000, 1)\n",
            "theta shape: (4582, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare to train the model\n"
      ],
      "metadata": {
        "id": "p-bxFoeJuSbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the logistic regression model\n",
        "alpha = 0.01\n",
        "num_iters = 1000\n",
        "theta, J_history = gradientDescent(X_train, y_train, theta, alpha, num_iters)"
      ],
      "metadata": {
        "id": "KFQsdoRquTPv"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict sentiment for the test set\n",
        "y_pred = sigmoid(np.dot(X_test, theta))\n",
        "y_pred_labels = np.zeros(y_pred.shape)\n",
        "for i in range(len(y_pred)):\n",
        "    if y_pred[i] >= 0.5:\n",
        "        y_pred_labels[i] = 1\n",
        "    else:\n",
        "        y_pred_labels[i] = 0\n",
        "y_pred_prob = sigmoid(np.dot(X_test, theta))  # Get probabilities\n",
        "y_pred_labels = (y_pred_prob >= 0.5).astype(int)  # Convert probabilities to binary labels"
      ],
      "metadata": {
        "id": "dObHywvXvuak"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_labels = (y_pred > 0.5).astype(int)  # Adjust the threshold if necessary\n",
        "\n",
        "# Convert y_test to binary labels (1 for positive, 0 for negative)\n",
        "y_test_binary = np.where(y_test == 0.5, 1, y_test).astype(int).flatten()\n",
        "\n",
        "# Ensure both y_test and y_pred_labels are of the same shape and type\n",
        "print(\"Shape of y_test:\", y_test.shape)\n",
        "print(\"Shape of y_pred_labels:\", y_pred_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aAwPfxKx2_y",
        "outputId": "497c56a9-15b2-43aa-a2fb-080425fb5702"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of y_test: (200, 1)\n",
            "Shape of y_pred_labels: (200, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test_binary, y_pred_labels.flatten())\n",
        "confusion = confusion_matrix(y_test_binary, y_pred_labels.flatten())\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko-pTHNs5cNG",
        "outputId": "a71d960d-5067-4746-f1f8-cb36635016e9"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.84\n",
            "Confusion Matrix:\n",
            "[[  4  26]\n",
            " [  6 164]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8Y7c4fZ+QNAc0qQ6cLeEP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}